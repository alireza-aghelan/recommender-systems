{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tWDue9oggaLR"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIrhFgC9gs8S",
        "outputId": "081a98c8-7cde-4d54-d03c-a684227dc71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m767.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/u/0/uc?id=1mipJf49ZvqD3CArtutXNOJriqdtb9tAH&export=download\n",
            "From (redirected): https://drive.google.com/uc?id=1mipJf49ZvqD3CArtutXNOJriqdtb9tAH&export=download&confirm=t&uuid=ae5a09dd-9aaa-4591-9c39-d65d1e52611b\n",
            "To: /content/dataset.zip\n",
            "100% 26.3M/26.3M [00:00<00:00, 71.2MB/s]\n",
            "patool: Extracting /content/dataset.zip ...\n",
            "patool: running /usr/bin/7z x -o/content/unzipped_folder -- /content/dataset.zip\n",
            "patool: ... /content/dataset.zip extracted to `/content/unzipped_folder'.\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!pip install --upgrade gdown\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "os.mkdir('unzipped_folder')\n",
        "\n",
        "!gdown \"https://drive.google.com/u/0/uc?id=1mipJf49ZvqD3CArtutXNOJriqdtb9tAH&export=download\"\n",
        "\n",
        "patoolib.extract_archive(\"/content/dataset.zip\", outdir=\"/content/unzipped_folder\")\n",
        "\n",
        "ratings = pd.read_csv(\"/content/unzipped_folder/dataset/rating.csv\")\n",
        "animes = pd.read_csv(\"/content/unzipped_folder/dataset/anime.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-q45gX86hRiZ"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "N = len(animes['name'])\n",
        "id_dic = dict(zip(list(range(N)), (animes[\"anime_id\"])))\n",
        "\n",
        "def clean_data(x):\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
        "    else:\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "\n",
        "features = ['name', 'genre', 'type', 'rating']\n",
        "\n",
        "for feature in features:\n",
        "\n",
        "    animes[feature] = animes[feature].apply(clean_data)\n",
        "\n",
        "\n",
        "def create_soup(x):\n",
        "  \n",
        "    return ' ' + x['name'] + ' ' + x['genre'] + ' ' + x['type'] + ' ' + x['rating']\n",
        "\n",
        "\n",
        "animes['soup'] = animes.apply(create_soup, axis=1)\n",
        "\n",
        "count = CountVectorizer(stop_words='english')\n",
        "count_matrix = count.fit_transform(animes['soup'])\n",
        "\n",
        "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
        "\n",
        "indices = pd.Series(animes.index, index=animes['anime_id']).drop_duplicates()\n",
        "\n",
        "def get_animes(user_id):\n",
        "\n",
        "  select = ratings.loc[ratings['user_id'] == user_id]\n",
        "  sel = select[['anime_id', 'rating']]\n",
        "  sel_sorted = sel.sort_values(by = 'rating', ascending = False)\n",
        "  all_animes = sel_sorted['anime_id'].tolist()\n",
        "  top_animes = all_animes[0:20]\n",
        "\n",
        "  return top_animes , all_animes\n",
        "\n",
        "\n",
        "def get_recommendation_content_based(top_animes, all_animes, cosine_sim=cosine_sim2):\n",
        "\n",
        "  rec_animes = []\n",
        "\n",
        "  for i in top_animes:\n",
        "\n",
        "    idx = indices[i]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:6]\n",
        "\n",
        "    animes_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    for j in animes_indices:\n",
        "\n",
        "      anime_id = id_dic[j]\n",
        "\n",
        "      if (anime_id not in rec_animes) and (anime_id not in all_animes):\n",
        "        rec_animes.append(anime_id)\n",
        "\n",
        "  return rec_animes\n",
        "\n",
        "\n",
        "def get_similar_ids(top_animes, all_animes, cosine_sim=cosine_sim2):\n",
        "\n",
        "  sim_ids = []\n",
        "\n",
        "  for i in top_animes:\n",
        "\n",
        "    idx = indices[i]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:41]\n",
        "\n",
        "    animes_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    for j in animes_indices:\n",
        "\n",
        "      anime_id = id_dic[j]\n",
        "      \n",
        "      if (anime_id not in sim_ids) and (anime_id not in all_animes):\n",
        "        sim_ids.append(anime_id)\n",
        "\n",
        "  return sim_ids\n",
        "\n",
        "\n",
        "animes_2 = pd.read_csv(\"/content/unzipped_folder/dataset/anime.csv\")\n",
        "ids_names = dict(zip(animes_2['anime_id'], animes_2['name'])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGxnpqDrjx4N",
        "outputId": "c5418b13-9ef9-4721-fa71-7d255f0ee17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter user id: 3\n",
            "\n",
            "Recommended animes:\n",
            "\n",
            "Pokemon: Mewtwo no Gyakushuu\n",
            "Pokemon: Kesshoutou no Teiou Entei\n",
            "Fullmetal Alchemist\n",
            "Mirai Nikki (TV)\n",
            "Clannad: After Story\n",
            "Clannad\n",
            "Kokoro Connect\n",
            "Durarara!!\n",
            "Kuroko no Basket: Tip Off\n",
            "Shokugeki no Souma: Ni no Sara\n"
          ]
        }
      ],
      "source": [
        "user_id = int(input(\"Enter user id: \"))\n",
        "\n",
        "top_animes, all_animes = get_animes(user_id)\n",
        "\n",
        "similar_ids = get_similar_ids(top_animes, all_animes, cosine_sim2)\n",
        "anime_ids = similar_ids + all_animes\n",
        "\n",
        "new_ratings = ratings.loc[ratings['anime_id'].isin(anime_ids)]\n",
        "new_ratings = new_ratings[new_ratings.rating != -1]\n",
        "\n",
        "def get_top_rated_animes(user_id):\n",
        "\n",
        "  select = new_ratings.loc[new_ratings['user_id'] == user_id]\n",
        "  sel = select[['anime_id', 'rating']]\n",
        "  sel_sorted = sel.sort_values(by = 'rating', ascending = False)\n",
        "  rated_animes = sel_sorted['anime_id'].tolist()\n",
        "  top_rated_animes = rated_animes[0:10]\n",
        "\n",
        "  return top_rated_animes\n",
        "\n",
        "\n",
        "def create_matrix(df):\n",
        "\t\n",
        "\tN = len(df['user_id'].unique())\n",
        "\tM = len(df['anime_id'].unique())\n",
        "\t\n",
        "\tuser_mapper = dict(zip(np.unique(df[\"user_id\"]), list(range(N))))\n",
        "\tanime_mapper = dict(zip(np.unique(df[\"anime_id\"]), list(range(M))))\n",
        "\t\n",
        "\tuser_inv_mapper = dict(zip(list(range(N)), np.unique(df[\"user_id\"])))\n",
        "\tanime_inv_mapper = dict(zip(list(range(M)), np.unique(df[\"anime_id\"])))\n",
        "\t\n",
        "\tuser_index = [user_mapper[i] for i in df['user_id']]\n",
        "\tanime_index = [anime_mapper[i] for i in df['anime_id']]\n",
        "\n",
        "\tX = csr_matrix((df[\"rating\"], (anime_index, user_index)), shape=(M, N))\n",
        "\t\n",
        "\treturn X, user_mapper, anime_mapper, user_inv_mapper, anime_inv_mapper\n",
        "\n",
        "\n",
        "X, user_mapper, anime_mapper, user_inv_mapper, anime_inv_mapper = create_matrix(new_ratings)\n",
        "\n",
        "kNN = NearestNeighbors(n_neighbors=10, algorithm=\"brute\", metric='cosine')\n",
        "kNN.fit(X)\n",
        "\n",
        "def find_similar_animes(anime_id, k, show_distance=False):\n",
        "      \n",
        "    neighbour_ids = []\n",
        "\n",
        "    anime_ind = anime_mapper[anime_id]\n",
        "    anime_vec = X[anime_ind]\n",
        "    anime_vec = anime_vec.reshape(1,-1)\n",
        "\n",
        "    neighbour = kNN.kneighbors(anime_vec, return_distance=show_distance)\n",
        "\n",
        "    for i in range(0,k):\n",
        "\n",
        "        n = neighbour.item(i)\n",
        "        neighbour_ids.append(anime_inv_mapper[n])\n",
        "\n",
        "    neighbour_ids.pop(0)\n",
        "    \n",
        "    return neighbour_ids\n",
        "\n",
        "\n",
        "act_users = new_ratings['user_id'].unique().tolist()\n",
        "\n",
        "print(\"\")\n",
        "print(\"Recommended animes:\")\n",
        "print(\"\")\n",
        "\n",
        "if user_id in act_users:\n",
        "  \n",
        "  top_rated_animes = get_top_rated_animes(user_id)\n",
        "\n",
        "  recommended_animes_comb = []\n",
        "  \n",
        "  for i in top_rated_animes:\n",
        "\n",
        "      similar_animes = find_similar_animes(i, k=10)\n",
        "\n",
        "      for j in similar_animes:\n",
        "        \n",
        "          if (j not in recommended_animes_comb) and (j not in all_animes):\n",
        "            recommended_animes_comb.append(j)\n",
        "\n",
        "  recommended_animes_comb = recommended_animes_comb[0:10]\n",
        "  \n",
        "  for i in recommended_animes_comb:\n",
        "    \n",
        "    print(ids_names[i])\n",
        "\n",
        "\n",
        "else :\n",
        "\n",
        "  top_animes_1 = all_animes[0:5]\n",
        "  recommended_animes_content_based = get_recommendation_content_based(top_animes_1, all_animes, cosine_sim2)[0:10]\n",
        "\n",
        "  for i in recommended_animes_content_based:\n",
        "\n",
        "\t  print(ids_names[i])  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}